<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="shortcut icon" type="image/png" href="./images/AMIcon.png" />
  <link rel="stylesheet" href="index.css" />

  <!-- Put your site title here -->
  <title>
    Aneil Maharaj | Machine Learning
  </title>

  <meta name="description" content="Add small description of yourself">
  <!-- Add some coding keywords below, Ex: (React, CSS etc) -->
  <meta name="keywords" content="Put your name, skills and some coding keywords" />
  <link rel="stylesheet" href="index.css" />
</head>
<body>
	<header class="header" role="banner" id="top">
    <div class="row">
      <nav class="nav" role="navigation">
        <ul class="nav__items">
          <li class="nav__item"><a href="https://amaharaj92.github.io" class="nav__link">Home</a></li>          
        </ul>
      </nav>
    </div>
    <div class="header__text-box row">
      <h1>My ePortfolio for Machine Learning.</h1>
        <div class="header__text">
    </div>
      </div>
  </header>

  <main role="main">
	  <h2>Unit 1</h2>
	  <h2>Unit 2</h2>
	  <h2>Unit 3</h2>
		<ul class="work__list">
			<li>Covariance Pearson Correlation</li><p>Pearson‚Äôs correlation is the deviation of the two values (x and y) which are multiplied, summed and divided by the total number of pairs of values minus 1 (n-1). This helps to calculate the difference between two variables and calculate whether the correlation is high or not, a score of 1 or near 1 indicates that there is a strong relationship between the 2 variables, while a score of -1 or near -1 indicates the absence or weak relationship between the 2 variables. From the code provided and following manipulation, it was found that reducing the number of points on the graph will reduce the accuracy of the mean for both data1 and data2 as there would be less values to be used to calculate the mean.</p> 
			<li>Linear Regression</li><p>Myfunc puts the calculations in the form of a straight line y =mx+c. The prediction uses the slope calculated, in this case the value of m to find the value of y. This is accomplished by using the stats.linregress which performs a linear least squares regression calculation on two arrays of values.</p>
			<li>Multiple Linear Regression</li><p>This technique uses variables to predict the outcome of a response variable. In this scenario, the code provided utilizes the weight of the vehicle and the volume of the displacement in order to predict the CO<sub>2</sub> output level. The dependent variables are the weight and volume while the dependent variable is CO<sub>2</sub>.</p>
			<li>Polynomial Regression</li><p>Polynomial regression is a form of regression analysis that allows for more complex patters or relationships to be made utilizing independent variables and also for predicting values. In this scenario for the code provided, contains two arrays for x and y values, where each value is mapped to create a curved line, with the x value indicating the time the vehicle passed and y the speed recorded at which the vehicle passed the booth. The value of r^2 indicates whether there is a relation and, in this scenario, a value of 0.94 indicates that there is a strong relationship in the model. This means that predictions can be made utilizing the regression model.</p>
		</ul>
		<h3>Legal, Social, Ethical & Professional Issues that Machine Learning Professionals Will Encounter</h3>
		    <ul class="work__list">
			<li>Legal</li><p>There are volumes of data available for individuals to utilize and learn from, however this does not mean that they can be used as a repository to train a data model without the publisher or author‚Äôs permission, especially those of copyrighted or protected data. There can be serious consequences associated with that from a legal background, ranging from jail time, to monetary fines, or even revoking of rights for data usage. Numerous websites and companies have incorporated a feature on their websites to prevent web scrapers from being able to collect data.</p>
			<li>Social</li><p>The social consequences of gathering data stem from where the data is collected, in some instances individuals are unaware that they are being recorded, and what the purpose of these recordings are. There are numerous cameras recording individuals however majority are for security reasons, however if individuals are having their likeness recorded to be used in a data model, there would be objections throughout. It is imperative that individuals are aware and give permission for this to occur.</p>
			<li>Ethical</li><p>There are many ethical considerations to be had when dealing with machine learning, many of which stem from bias and fairness. Unfortunately the data which a model may be trained upon may have inherent bias that was subconsciously achieved. In instances this like this, the model will not function correctly and it is imperative for the model to have this bias mitigated, by using diverse datasets when training is being done. In addition to diversity in the datasets, ongoing adjustments and monitoring should be the standard to overcome these biases, such as in facial recognition software where darker skin tones have a direct relation to higher error rates.</p>
			<li>Professional</li><p>Professional issues which occur from the use of machine learning stem directly from the issue of individuals utilizing technology to accomplish all of, or majority of their work. In the instance of education and scholars, individuals can use machine learning, not to calculate results, but to write entire papers and submitting it as their own, unfortunately this relates to the first issue discussed, which constitutes the legal right to utilize the data, and the ethical right to use another individual‚Äôs data and pass it off as their own, otherwise known as plagiarism.</p>
		    </ul>
	  	<h3>Machine Learning Algorithms</h3>
	  		<p>There are numerous machine learning algorithms which utilize various methods of creating models, including deep learning, decision trees or suppor vector machines. They are defined by</p>
	  	<ul class="work__list">
			<li>The use of neural networks on a dataset is referred to as deep learning and is popular for image recognition, natural language processing and speech recognition. This type of network utilizes layers to split the data in order to train the model and as a result output accurate output.  The datasets which are utilized in neural networks include image datasets and audio datasets. </li>
			<li>The decision tree approach is a supervised learning approach which utilizes flow charts created by the computer to model the dataset on ad predict values based on the probabilities of each branch occurring from a past outcome, indicating that there must be some level of history for this to be based on. This is primarily utilized for classification and regression tasks. The datasets which are utilized for this type of machine learning would be those of stock trading or types of financial transactions. </li>
			<li>The support vector machine approach utilized supervised learning in order to classify binary groups or solve regression and outlier tasks. It is primarily good for text classification, image and handwriting classification, spam detection and gene expression analysis to give a few examples. Their effectiveness stems from the finding the line that separates datapoints to create the classification groups.  </li>
		</ul>
	  <h3>Unit 4</h3>
	  <h3>Unit 5</h3>
		<img
		    src="./images/Tabl31.png
		    class="work__image"
		    alt="Project 1"
		/>
	  <h3>Unit 6</h3>
	  <h3>Unit 7</h3>
	  <h3>Unit 8</h3>
	  	<p>Mayo 2017 deals with an optimization algorithm, specifically, the gradient descent which is utilized for increasing the data model‚Äôs efficiency by increasing accuracy while reducing errors. The author goes on to say that most algorithms are based of linear regression algorithms, and gradient descent works well for supervised learning. The gradient descent algorithm states that it will run until ‚Äúconvergence‚Äù meaning that it will continue until the values become so close that they are negligible, with this being achieved it will lower the cost function of the data model, thus increasing efficiency, this term is referred to as the global optimization factor. Its purpose is to minimize the cost function of the data model to ensure efficiency. As a result of this convergence indicating efficiency, the learning rate plays an important role as a learning rate that is too high, will result in an overshooting of the cost function minima, or potentially diverge (which is the opposite of what we are trying to achieve), while too low and it becomes inefficient by taking a longer period of time for convergence to occur. </p>
	  	<p>The gradient descent exercise when run with a learning rate of 0.01 and 0.03 resulted in costs of 0.480 and 0.125 respectively at 100 iterations. Conversely, running the same rate at 0.001 and 0.1 resulted in much greater costs such as 2.6 and 5.9 respectively after the 100th iteration. </p>
	  	<p>However, when reducing the number of iterations with learning rates of 0.01, 0.03, 0.001 and 0.1 resulted in costs of 1.57, 0.78, 58.16 and 24209.9 respectively, thus indicating smaller iterations will result in a significantly higher cost due to the lack of times the dataset is processed, however going for too many iterations could result in overfitting, unfortunately.</p>
	  <h3>Unit 9</h3>
	  <h3>Unit 10</h3> 
	  	<p>There are many ethical and social issues associated with machine learning when it comes to identifying individuals, majority of which stem from the source of the data to train the model for identifying individuals. There is a large issue associated with the capture of individuals likeness without their permission for training, and identifying them throughout the world. Individuals are never far away from a camera, and as a result of this their likeness is captured at minimum 100 times a day, depending on the advancement of technology in the country. An example of this would be passengers at a train station are recorded numerous times, from the ticketing booth, to their train stop, the boarding and disembarking of the train and eventual the departure of the station. These are examples of times where individuals do not think to ask the question, ‚Äúis my likeness going to be utilized for training these models?‚Äù</p>
	  	<p>Unfortunately, this raises an ethical consideration, of which many people do not realize, the fine print on purchasing a ticket giving the company permission to record the individuals, and utilize it as they see fit. This is very similar to the software license agreement which many people have blindly accepted in the past, unsure as to what they are accepting. Furthermore, there is a large discrepancy when comparing the color of skin tones when attempting to identify persons as there is a higher risk of error with darker skin tones, than lighter skin tones due to the limited dataset which may have been utilized to train the model, resulting in numerous wrongful incarcerations occurring, worldwide. </p>
	  	<p>The use of facial recognition and machine learning is something which is still new and it is still in development, however the consistent use of undertrained programs will continue to result in the minorities being affected due to the limited training sets available, however, it will also be difficult for individuals who already feel targeted to willingly succumb to submitting their likeness to train a model, which they already believe is targeting them. </p>
	  	<b>Simple Perceptron</b>
	  		<p>The simple perceptron is a neural network in its simplest form which can be utilized for pattern classification and information storage. It utilizes relative weights, an activation function and a bias to determine output based on the activation value. In this scenario the inputs are age, and the amount of work experience weighted at 0.7 and 0.1 respectively and the example is set at age 45 with 25 years of work experience, this equates to the summation of 34, and gives a probability of >1 (in this case 34) indicating that the sum function is greater than 1, which means the neuron is activated allowing for the individual to be passed. The usefulness of a perceptron comes from the binary results it produces allowing for simple classification into 2 categories. </p>
	   	<b>Perceptron and Operator</b>
	  		<p>The perceptron is another building block in neural networks and works by manipulating the logic and performing operations on the input. The training method is an iterative one (a for loop, until a condition is met) and its weights are adjusted after each iteration based on the predicted error, until this is reduced to zero. The learning rate variable determines how quickly the weights are updated, a higher rate means the model will update faster and a lower rate means the model will update slower. Neither one of these situations are ideal, and a middle rate is ideal, as a model trained too quickly will have its optimal solution surpassed, which results in poor performance on new data, and too slowly, will have the same effect, but due to the model not being trained enough. </p>
	   	<b>Multi Layer Perceptron</b>
	  		<p>The multi layer perceptron is an artificial neural network which consist of interconnected neurons organized in layers, and each neuron in the previous layer is connected to all the neurons in the current layer. These layers are the values associated with the weights which are used to train the model. The ideal use for this is for classification of images and regression, due to the complex relationship learning ability the models can be trained for. The model is trained utilizing the method of back propagation which corrects for errors when training the model. The key terms in the use of multi layer perceptron are:</p>
	 			<ul class="work__list">
					<li>Epoch ‚Äì Number of times the training dataset is processed. Increasing this allows for better performance, however it can result in the model being unable to differentiate new data. </li>
					<li>Learning Rate ‚Äì Rate at which the weights are updated however a rate that is too high can cause overfitting of data. </li>
					<li>Weights ‚Äì the weighting of the output and input layer will affect how the neural network performs </li>
				</ul>
	  	<p>The model performance measurement file was manipulated with different variables to see the effect on the AUC and R2 error, however these must first be defined. </p>
	  	<p>The AUC (Area Under Curve) score is calculated by means of plotting a graph and calculating the area under said curve and it represents the probability of how well the model can identify between positive and negative cases. </p>
	  	<p>R2 (R squared) error is defined as a regression metric used to represent the error, which relates to the model‚Äôs performance. There is a direct relation to the value of the R2 error when the number of independent variables increase. </p>
	  	<p>It was noted that the following variables/parameters were manipulated and observed to have had an impact on the AUC and R2 values:</p>
	  		<ul class="work__list">
				<li>The learning rates affect both due to the speed at which the model is updating the weighting following data processing of the dataset.</li>
				<li>The type of model utilized, is important because different datasets would have regression models ideally suited; increasing efficiency of the training and reducing errors. </li>
			</ul>
	  <h3>Unit 11</h3> 
	  <h3>Unit 12</h3> 
	  <h3>Reflective</h3>
	  	<p>Throughout this machine learning course it has been quite difficult to make time to sit and read as much as I have in my previous courses due to projects coming due in my professional life and my difficulty with understanding how to code. The use of numerous websites and the seminars via Google Colab notebooks came in very helpful and the knowledge which was garnered has already been put into practice in my professional life. I look forward to seeing how well I can implement and improve on items in my professional life. </p>
	  | Command | Description |
| --- | --- |
| git status | List all new or modified files |
| git diff | Show file differences that haven't been staged |
  </main>

  <!-- ***** Contact ***** -->

  <section class="contact" id="contact">
    <div class="row">
    </div>
  </section>

  <!-- ***** Footer ***** -->

      <!-- If you give me some credit by keeping the below paragraph, will be huge for me üòä Thanks. -->
      <p>
        &copy; 2020 - Template designed & developed by <a href="https://nisar.dev" class="link">Nisar</a>.
      </p>
      <div class="footer__github-buttons">
        <iframe
          src="https://ghbtns.com/github-btn.html?user=nisarhassan12&repo=portfolio-template&type=watch&count=true"
          frameborder="0" scrolling="0" width="170" height="20" title="Watch Portfolio Template on GitHub">
        </iframe>
      </div>
    </div>
  </footer>

  <a href="#top" class="back-to-top" title="Back to Top">
    <img src="./images/arrow-up.svg" alt="Back to Top" class="back-to-top__image"/>
  </a>
  <script src="./index.js"></script>
</body>

</html>
