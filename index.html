<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <link rel="shortcut icon" type="image/png" href="./images/AMIcon.png" />
  <link rel="stylesheet" href="index.css" />

  <!-- Put your site title here -->
  <title>
    Aneil Maharaj | Machine Learning
  </title>

  <meta name="description" content="Add small description of yourself">
  <!-- Add some coding keywords below, Ex: (React, CSS etc) -->
  <meta name="keywords" content="Put your name, skills and some coding keywords" />
  <link rel="stylesheet" href="index.css" />
</head>
<body>
	<header class="header" role="banner" id="top">
    <div class="row">
      <nav class="nav" role="navigation">
        <ul class="nav__items">
          <li class="nav__item"><a href="https://amaharaj92.github.io" class="nav__link">Home</a></li>          
        </ul>
      </nav>
    </div>
    <div class="header__text-box row">
      <h1>My ePortfolio for Machine Learning.</h1>
        <div class="header__text">
    </div>
      </div>
  </header>

  <main role="main">
	  <h2>Unit 1</h2>
	  	<p>Unit 1 dealt with an introduction to machine learning, and the theoretical aspect of what the course entails.</p>
	  	<p>The first collaborative discussion was assigned to speak about the Fourth Industrial Revolution or Industry 4.0" which is based on the incorporation of technology into industrial processes in order to improve efficiencies.</p>
	  	<p>We were also divided into groups in order to work on our group project which deals with an AirBnB dataset for New York City and requires a report to be presented to the management in order to make better informed business decisions.</p>
	  <h2>Unit 2</h2>
	  	<p>Exploratory Data Analysis (EDA) was introduced in the second unit, and the purpose of EDA is to spot any types of patterns in the dataset. The main items of the data are identified and interpreted. The data should ideally be cleaned, ie, removal of duplicates, removal of rows which have incomplete information, and grouping of relevant properties. This will be useful for the group assignment due to the number of items which contain similar characterisitics in the dataset.</p>
	  <h2>Unit 3</h2>
		<ul class="work__list">
			<li>Covariance Pearson Correlation</li><p>Pearson‚Äôs correlation is the deviation of the two values (x and y) which are multiplied, summed and divided by the total number of pairs of values minus 1 (n-1). This helps to calculate the difference between two variables and calculate whether the correlation is high or not, a score of 1 or near 1 indicates that there is a strong relationship between the 2 variables, while a score of -1 or near -1 indicates the absence or weak relationship between the 2 variables. From the code provided and following manipulation, it was found that reducing the number of points on the graph will reduce the accuracy of the mean for both data1 and data2 as there would be less values to be used to calculate the mean.</p> 
			<li>Linear Regression</li><p>Myfunc puts the calculations in the form of a straight line y =mx+c. The prediction uses the slope calculated, in this case the value of m to find the value of y. This is accomplished by using the stats.linregress which performs a linear least squares regression calculation on two arrays of values.</p>
			<li>Multiple Linear Regression</li><p>This technique uses variables to predict the outcome of a response variable. In this scenario, the code provided utilizes the weight of the vehicle and the volume of the displacement in order to predict the CO<sub>2</sub> output level. The dependent variables are the weight and volume while the dependent variable is CO<sub>2</sub>.</p>
			<li>Polynomial Regression</li><p>Polynomial regression is a form of regression analysis that allows for more complex patters or relationships to be made utilizing independent variables and also for predicting values. In this scenario for the code provided, contains two arrays for x and y values, where each value is mapped to create a curved line, with the x value indicating the time the vehicle passed and y the speed recorded at which the vehicle passed the booth. The value of r^2 indicates whether there is a relation and, in this scenario, a value of 0.94 indicates that there is a strong relationship in the model. This means that predictions can be made utilizing the regression model.</p>
		</ul>
		<h3>Legal, Social, Ethical & Professional Issues that Machine Learning Professionals Will Encounter</h3>
		    <ul class="work__list">
			<li>Legal</li><p>There are volumes of data available for individuals to utilize and learn from, however this does not mean that they can be used as a repository to train a data model without the publisher or author‚Äôs permission, especially those of copyrighted or protected data. There can be serious consequences associated with that from a legal background, ranging from jail time, to monetary fines, or even revoking of rights for data usage. Numerous websites and companies have incorporated a feature on their websites to prevent web scrapers from being able to collect data.</p>
			<li>Social</li><p>The social consequences of gathering data stem from where the data is collected, in some instances individuals are unaware that they are being recorded, and what the purpose of these recordings are. There are numerous cameras recording individuals however majority are for security reasons, however if individuals are having their likeness recorded to be used in a data model, there would be objections throughout. It is imperative that individuals are aware and give permission for this to occur.</p>
			<li>Ethical</li><p>There are many ethical considerations to be had when dealing with machine learning, many of which stem from bias and fairness. Unfortunately the data which a model may be trained upon may have inherent bias that was subconsciously achieved. In instances this like this, the model will not function correctly and it is imperative for the model to have this bias mitigated, by using diverse datasets when training is being done. In addition to diversity in the datasets, ongoing adjustments and monitoring should be the standard to overcome these biases, such as in facial recognition software where darker skin tones have a direct relation to higher error rates.</p>
			<li>Professional</li><p>Professional issues which occur from the use of machine learning stem directly from the issue of individuals utilizing technology to accomplish all of, or majority of their work. In the instance of education and scholars, individuals can use machine learning, not to calculate results, but to write entire papers and submitting it as their own, unfortunately this relates to the first issue discussed, which constitutes the legal right to utilize the data, and the ethical right to use another individual‚Äôs data and pass it off as their own, otherwise known as plagiarism.</p>
		    </ul>
	  	<h3>Machine Learning Algorithms</h3>
	  		<p>There are numerous machine learning algorithms which utilize various methods of creating models, including deep learning, decision trees or suppor vector machines. They are defined by</p>
	  	<ul class="work__list">
			<li>The use of neural networks on a dataset is referred to as deep learning and is popular for image recognition, natural language processing and speech recognition. This type of network utilizes layers to split the data in order to train the model and as a result output accurate output.  The datasets which are utilized in neural networks include image datasets and audio datasets. </li>
			<li>The decision tree approach is a supervised learning approach which utilizes flow charts created by the computer to model the dataset on ad predict values based on the probabilities of each branch occurring from a past outcome, indicating that there must be some level of history for this to be based on. This is primarily utilized for classification and regression tasks. The datasets which are utilized for this type of machine learning would be those of stock trading or types of financial transactions. </li>
			<li>The support vector machine approach utilized supervised learning in order to classify binary groups or solve regression and outlier tasks. It is primarily good for text classification, image and handwriting classification, spam detection and gene expression analysis to give a few examples. Their effectiveness stems from the finding the line that separates datapoints to create the classification groups.  </li>
		</ul>
	  <h2>Unit 4</h2>
	  	<p>This unit dealt with the introduction of regression utilizing Scikit Learn which is an open source python package. It is used for creating and interpreting models. The first step of which is to preprocess the data to ensure that the data will not have any outliers which could result in skewed values occurring. Following this, analysis is achieved by plotting points on a graph</p>
	  <h2>Unit 5 & 6</h2>
	  	<p>Prior to any calculations being done, positive results were assigned the binary value of 1, and negative values were assigned the value of 0. The following table shows the transfigured information from the test results of Jack, Mary and Jim:</p>
			<img
			    src="./images/Table1.png"
			    class="work__image"
			    alt="Table 1"
			/>
	  <p>The Jaccard coefficients for the pairs are as follows: </p>
	  <p><b>Jack and Mary</b> both have Fever, and positive results for Tet-1 while Mary tested positive for Test-3 resulting in a numerator of 2, and a denominator of 3. This gives 2/3 or 0.66‚Äô</p>
	  <p>Jack and Jim both have fever, while Jack tests positive for Test-1 and Jim has a cough resulting in a numerator of 1, and a denominator of 3. This gives 1/3 or 0.33.</p>
	  <p>Jim and Mary both have a fever, however Jim has a cough while Mary tests positive for Test-1 and Test-3 resulting in a numerator of 1 and a denominator of 4. This gives 1/4  or 0.25.</p>
	  <p>The clustering videos that were provided gave an insight into how the algorithm works visually which was very insightful. It provided an excellent example of how the model works to cluster groups of data while also showing how the centroids are adjusted to create the clusters of the groups.</p>
	  <h2>Unit 7</h2>
	  	<p>An Artifical Nueral Network takes inspiration from Biology due to the way data is processed and adaptationg of the data model following training. It is regarded as an important contributor to Industry 4.0 as it utilizes interconnected nodes on different layers to ensure that the data model can process the data and make an informed decision. The introduction of this is important to the upcoming assignment as an artificial neural network has to be designed. </p>
	  <h2>Unit 8</h2>
	  	<p>Mayo 2017 deals with an optimization algorithm, specifically, the gradient descent which is utilized for increasing the data model‚Äôs efficiency by increasing accuracy while reducing errors. The author goes on to say that most algorithms are based of linear regression algorithms, and gradient descent works well for supervised learning. The gradient descent algorithm states that it will run until ‚Äúconvergence‚Äù meaning that it will continue until the values become so close that they are negligible, with this being achieved it will lower the cost function of the data model, thus increasing efficiency, this term is referred to as the global optimization factor. Its purpose is to minimize the cost function of the data model to ensure efficiency. As a result of this convergence indicating efficiency, the learning rate plays an important role as a learning rate that is too high, will result in an overshooting of the cost function minima, or potentially diverge (which is the opposite of what we are trying to achieve), while too low and it becomes inefficient by taking a longer period of time for convergence to occur. </p>
	  	<p>The gradient descent exercise when run with a learning rate of 0.01 and 0.03 resulted in costs of 0.480 and 0.125 respectively at 100 iterations. Conversely, running the same rate at 0.001 and 0.1 resulted in much greater costs such as 2.6 and 5.9 respectively after the 100th iteration. </p>
	  	<p>However, when reducing the number of iterations with learning rates of 0.01, 0.03, 0.001 and 0.1 resulted in costs of 1.57, 0.78, 58.16 and 24209.9 respectively, thus indicating smaller iterations will result in a significantly higher cost due to the lack of times the dataset is processed, however going for too many iterations could result in overfitting, unfortunately.</p>
	  <h2>Unit 9</h2>
	  	<p>The introduction of a convolutional neural network was accomplished, and this is a specialized artificial neural network which has a primary focus on object recognition. We were introduced to the CIFAR 10 dataset, and how a CNN works buy dividing the images into pixels, and adding layers to train the data model to identify images from a dataset. This was quite interesting to see how object recognition occurs, however it was daunting as well due to the incorrect recognition of simple items, especially when it is being used for identifying felons. It was definitely an eye opener to some of the ethical considerations of technology which is ever evolving at an exponential rate.</p>
	  <h2>Unit 10</h2> 
	  	<p>There are many ethical and social issues associated with machine learning when it comes to identifying individuals, majority of which stem from the source of the data to train the model for identifying individuals. There is a large issue associated with the capture of individuals likeness without their permission for training, and identifying them throughout the world. Individuals are never far away from a camera, and as a result of this their likeness is captured at minimum 100 times a day, depending on the advancement of technology in the country. An example of this would be passengers at a train station are recorded numerous times, from the ticketing booth, to their train stop, the boarding and disembarking of the train and eventual the departure of the station. These are examples of times where individuals do not think to ask the question, ‚Äúis my likeness going to be utilized for training these models?‚Äù</p>
	  	<p>Unfortunately, this raises an ethical consideration, of which many people do not realize, the fine print on purchasing a ticket giving the company permission to record the individuals, and utilize it as they see fit. This is very similar to the software license agreement which many people have blindly accepted in the past, unsure as to what they are accepting. Furthermore, there is a large discrepancy when comparing the color of skin tones when attempting to identify persons as there is a higher risk of error with darker skin tones, than lighter skin tones due to the limited dataset which may have been utilized to train the model, resulting in numerous wrongful incarcerations occurring, worldwide. </p>
	  	<p>The use of facial recognition and machine learning is something which is still new and it is still in development, however the consistent use of undertrained programs will continue to result in the minorities being affected due to the limited training sets available, however, it will also be difficult for individuals who already feel targeted to willingly succumb to submitting their likeness to train a model, which they already believe is targeting them. </p>
	  	<b>Simple Perceptron</b>
	  		<p>The simple perceptron is a neural network in its simplest form which can be utilized for pattern classification and information storage. It utilizes relative weights, an activation function and a bias to determine output based on the activation value. In this scenario the inputs are age, and the amount of work experience weighted at 0.7 and 0.1 respectively and the example is set at age 45 with 25 years of work experience, this equates to the summation of 34, and gives a probability of >1 (in this case 34) indicating that the sum function is greater than 1, which means the neuron is activated allowing for the individual to be passed. The usefulness of a perceptron comes from the binary results it produces allowing for simple classification into 2 categories. </p>
	   	<b>Perceptron and Operator</b>
	  		<p>The perceptron is another building block in neural networks and works by manipulating the logic and performing operations on the input. The training method is an iterative one (a for loop, until a condition is met) and its weights are adjusted after each iteration based on the predicted error, until this is reduced to zero. The learning rate variable determines how quickly the weights are updated, a higher rate means the model will update faster and a lower rate means the model will update slower. Neither one of these situations are ideal, and a middle rate is ideal, as a model trained too quickly will have its optimal solution surpassed, which results in poor performance on new data, and too slowly, will have the same effect, but due to the model not being trained enough. </p>
	   	<b>Multi Layer Perceptron</b>
	  		<p>The multi layer perceptron is an artificial neural network which consist of interconnected neurons organized in layers, and each neuron in the previous layer is connected to all the neurons in the current layer. These layers are the values associated with the weights which are used to train the model. The ideal use for this is for classification of images and regression, due to the complex relationship learning ability the models can be trained for. The model is trained utilizing the method of back propagation which corrects for errors when training the model. The key terms in the use of multi layer perceptron are:</p>
	 			<ul class="work__list">
					<li>Epoch ‚Äì Number of times the training dataset is processed. Increasing this allows for better performance, however it can result in the model being unable to differentiate new data. </li>
					<li>Learning Rate ‚Äì Rate at which the weights are updated however a rate that is too high can cause overfitting of data. </li>
					<li>Weights ‚Äì the weighting of the output and input layer will affect how the neural network performs </li>
				</ul>
	  	<p>The model performance measurement file was manipulated with different variables to see the effect on the AUC and R2 error, however these must first be defined. </p>
	  	<p>The AUC (Area Under Curve) score is calculated by means of plotting a graph and calculating the area under said curve and it represents the probability of how well the model can identify between positive and negative cases. </p>
	  	<p>R2 (R squared) error is defined as a regression metric used to represent the error, which relates to the model‚Äôs performance. There is a direct relation to the value of the R2 error when the number of independent variables increase. </p>
	  	<p>It was noted that the following variables/parameters were manipulated and observed to have had an impact on the AUC and R2 values:</p>
	  		<ul class="work__list">
				<li>The learning rates affect both due to the speed at which the model is updating the weighting following data processing of the dataset.</li>
				<li>The type of model utilized, is important because different datasets would have regression models ideally suited; increasing efficiency of the training and reducing errors. </li>
			</ul>
	  <h2>Unit 11</h2> 
	  	<p>The most important aspect of machine learning is the selection of a data model as this has a direct bearing on performance based on both training and new data to be processed. Unit 11 focussed on this and stressed on the importance of choosing the right model based on the dataset, while also introducing the balance of bias and variance for the data model. In order to create a model which will excel, utilization of different datasets for training, manipulating a single model, and using a combination of models should be implemented.</p>
	  <h2>Unit 12</h2> 
	  	<p>The end of the course came  full circle with Unit 12 revisiting Industry 4.0 which was the first collaborative discussion completed. The evolution of the world is the only constant and with the introduction of faster, more efficient technology, there will only be more machine learning and artificial intelligence utilized across numerous industries. Important to note is the job market worry as there have already been replacements of man by machine in many factories worldwide. Unfortunately for each positive, there is a negative, for example machines are replaceable, humans are not, meaning that if a machine is damaged during routine work, it can be repaired whereas a human cannot (Cynically speaking that is). It is important to note however that there should be continuous monitoring of machinery during these conditions as machines while efficient are not always perfect. </p>
	  	<p>Due to my professional background in aviation, I have gone for a prediction model; logistic regression, based on linear regression due to the fact that majority of aircraft parts are stress tested over time periods to ensure that they can withstand the enormous strain that is placed on them at an almost daily basis. The thought process behind this is due to the inspection of the part, and if required corrective maintenance required. Currently items are tracked either by calendar or flight hour time and these are based off of predictions by the original equipment manufacturer or if they have received feedback from customers. Some items also have sensors and gyroscopes which record vibrations and when connected to the internet can be downloaded, further enhancing efficiency via the Internet of Things solution.</p>
	  	<p>In addition to this, it can also predict the preventative maintenance of parts in an attempt to assist the maintenance department produce airworthy aircraft for the customer and client. </p>
	  <h2>Reflective</h2>
	  	<p>Throughout this machine learning course it has been quite difficult to make time to sit and read as much as I have in my previous courses due to projects coming due in my professional life and my difficulty with understanding how to code. The use of numerous websites and the seminars via Google Colab notebooks came in very helpful and the knowledge which was garnered has already been put into practice in my professional life. I look forward to seeing how well I can implement and improve on items in my professional life. </p>
	  	<p>While this course opened my eyes to the wonders of Artificial Intelligence and pulled the curtain back on how it is achieved, I am personally still not convinced to use A.I. in my personal life as there are too many considerations, and lack of understanding on my part to be truly comfortable with utilization of a very useful, but concerning evolution in technology.</p>
  </main>

  <!-- ***** Contact ***** -->

  <section class="contact" id="contact">
    <div class="row">
    </div>
  </section>

  <!-- ***** Footer ***** -->

      <!-- If you give me some credit by keeping the below paragraph, will be huge for me üòä Thanks. -->
      <p>
        &copy; 2020 - Template designed & developed by <a href="https://nisar.dev" class="link">Nisar</a>.
      </p>
      <div class="footer__github-buttons">
        <iframe
          src="https://ghbtns.com/github-btn.html?user=nisarhassan12&repo=portfolio-template&type=watch&count=true"
          frameborder="0" scrolling="0" width="170" height="20" title="Watch Portfolio Template on GitHub">
        </iframe>
      </div>
    </div>
  </footer>

  <a href="#top" class="back-to-top" title="Back to Top">
    <img src="./images/arrow-up.svg" alt="Back to Top" class="back-to-top__image"/>
  </a>
  <script src="./index.js"></script>
</body>

</html>
